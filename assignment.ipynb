{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p8LjLtSqYqu",
        "outputId": "f83eee7b-b237-4b7d-a63f-6dae8562ac98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "df = pd.read_csv('train.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploring the dataset"
      ],
      "metadata": {
        "id": "J7BQeFzjR5-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()['Context']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-dKK_mTFh9g",
        "outputId": "81334214-a427-4258-9a5b-980fe5f67a64"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I'm going through some things with my feelings...\n",
              "1    I'm going through some things with my feelings...\n",
              "2    I'm going through some things with my feelings...\n",
              "3    I'm going through some things with my feelings...\n",
              "4    I'm going through some things with my feelings...\n",
              "Name: Context, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()['Response']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kmjPAwdFnTF",
        "outputId": "6d5c6880-3120-4957-bede-1d352d2703bb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    If everyone thinks you're worthless, then mayb...\n",
              "1    Hello, and thank you for your question and see...\n",
              "2    First thing I'd suggest is getting the sleep y...\n",
              "3    Therapy is essential for those that are feelin...\n",
              "4    I first want to let you know that you are not ...\n",
              "Name: Response, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = df['Response'].astype(str)"
      ],
      "metadata": {
        "id": "Z08gyQPVGoB8"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CKCwJpQKQZ2",
        "outputId": "7b0d851c-580e-4fdf-a487-01fe1013785d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    If everyone thinks you're worthless, then mayb...\n",
              "1    Hello, and thank you for your question and see...\n",
              "2    First thing I'd suggest is getting the sleep y...\n",
              "3    Therapy is essential for those that are feelin...\n",
              "4    I first want to let you know that you are not ...\n",
              "Name: Response, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the sentences into space separated words and punctuations. Putting a space between punctuation and word basically."
      ],
      "metadata": {
        "id": "5wRSsQqpSAoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_data(text):\n",
        "    text = text.lower()\n",
        "    text = re.findall(r'\\w+|[^\\s\\w]+', text)\n",
        "    text = ' '.join(word for word in text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "UQy3lmPBA5uL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [create_data(x) for x in data]"
      ],
      "metadata": {
        "id": "YsdPMNRdXDgP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finding out the symbols present in the dataset"
      ],
      "metadata": {
        "id": "73gMEnd3SKrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "puncs = dict()\n",
        "\n",
        "for sentence in data:\n",
        "    result = re.findall(r'[!\"\\$%&\\'()*+,\\-.\\/:;=#@?\\[\\\\\\]^_`{|}~]*', sentence)\n",
        "    for r in result:\n",
        "        for ch in r:\n",
        "            if ch in puncs:\n",
        "                puncs[ch] += 1\n",
        "            else:\n",
        "                puncs[ch] = 1\n",
        "\n",
        "puncs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIbHZJHgXHvM",
        "outputId": "39851276-4f57-4d68-da35-0b2c8a5ea77d"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{\"'\": 9182,\n",
              " ',': 24750,\n",
              " '.': 33642,\n",
              " '-': 3145,\n",
              " '!': 1394,\n",
              " '\"': 3663,\n",
              " '#': 10,\n",
              " ';': 572,\n",
              " ':': 1075,\n",
              " ')': 1682,\n",
              " '(': 1459,\n",
              " '?': 3072,\n",
              " '/': 2966,\n",
              " '%': 54,\n",
              " '&': 68,\n",
              " '[': 15,\n",
              " ']': 15,\n",
              " '$': 13,\n",
              " '*': 19,\n",
              " '~': 38,\n",
              " '_': 73,\n",
              " '=': 23,\n",
              " '+': 1445,\n",
              " '{': 4,\n",
              " '}': 4,\n",
              " '@': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing symbols that are not punctuations or are statistically insignificant due to their low frequency."
      ],
      "metadata": {
        "id": "dSxWtuuJSOTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "REMOVE_SYM = re.compile('[*#+~_%&={}@]$\\[\\]]')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = REMOVE_SYM.sub('', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Mz_rIlsAKej1"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [clean_text(x) for x in data]"
      ],
      "metadata": {
        "id": "pTaLXtjGLQ4c"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The space-separated and cleaned data."
      ],
      "metadata": {
        "id": "Oi4h0XuiSW9j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "gREwQpI2MRqk",
        "outputId": "461874ec-c9d3-461e-a0e4-0c9635fd4755"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if everyone thinks you ' re worthless , then maybe you need to find new people to hang out with . seriously , the social context in which a person lives is a big influence in self - esteem . otherwise , you can go round and round trying to understand why you ' re not worthless , then go back to the same crowd and be knocked down again . there are many inspirational messages you can find in social media . maybe read some of the ones which state that no person is worthless , and that everyone has a good purpose to their life . also , since our culture is so saturated with the belief that if someone doesn ' t feel good about themselves that this is somehow terrible . bad feelings are part of living . they are the motivation to remove ourselves from situations and relationships which do us more harm than good . bad feelings do feel terrible . your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today .\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the lines into tokens."
      ],
      "metadata": {
        "id": "xmLNZ6wlSbpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [x.split() for x in data]"
      ],
      "metadata": {
        "id": "0rA9nnWk3L1m"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uI4mBBNh3fCh",
        "outputId": "bda6aa88-96e6-462f-fa63-8afe87705f67"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['if', 'everyone', 'thinks', 'you', \"'\"]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Flattening the list"
      ],
      "metadata": {
        "id": "904FDjuOSgPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [x for xs in data for x in xs]"
      ],
      "metadata": {
        "id": "Ewax8UMA3tpL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "n_YfqaUR3_jE",
        "outputId": "3bec94c1-86db-4911-8dd7-df14a9578d8d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'if'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Label data according to the following labeling style:\n",
        "For any word if the following word is a punctuation label it with the tag of the punctuation, otherwise provide the default tag 'O'."
      ],
      "metadata": {
        "id": "ZMlUewcRRfAG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged = []\n",
        "\n",
        "pdict = {'.': 'PERIOD', ',': 'COMMA', '!': 'EXCL', '-': 'HYPHEN', '\"': 'DQUOTE', \"'\": 'SQUOTE', ';': 'SCOLON', ':': 'COLON', '(' : 'PSTART', ')' : 'PEND', '?': 'QUESTION', '/': 'SLASH'}\n",
        "\n",
        "for i in range(0, len(data) - 1):\n",
        "    if (data[i] not in pdict):\n",
        "        if (data[i + 1] in pdict):\n",
        "            tagged.append([data[i], pdict[data[i + 1]]])\n",
        "        else:\n",
        "            tagged.append([data[i], 'O'])"
      ],
      "metadata": {
        "id": "fXwF-3qF4FIR"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample"
      ],
      "metadata": {
        "id": "Mw4ljwHmRbwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tagged[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQPBP4UM7LdV",
        "outputId": "e25d930c-bcd4-4ce4-8cbc-97c2b14c8155"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'SQUOTE']"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Size of dataset"
      ],
      "metadata": {
        "id": "G8FjGah3RYq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len (tagged)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJjpUSU07s9O",
        "outputId": "14e291c3-aa72-45a4-d184-41fceee271d1"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "650463"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initializing token style for BERT model"
      ],
      "metadata": {
        "id": "3IRtmf2vRRoi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import *\n",
        "\n",
        "TOKEN_IDX = {\n",
        "    'bert': {\n",
        "        'START_SEQ': 101,\n",
        "        'PAD': 0,\n",
        "        'END_SEQ': 102,\n",
        "        'UNK': 100\n",
        "    }\n",
        "}\n",
        "\n",
        "punctuation_dict = {'O': 0, 'PERIOD': 1, 'COMMA': 2, 'EXCL': 3, 'HYPHEN' : 4, 'DQUOTE' : 5, 'SQUOTE': 6, 'SCOLON' : 7, 'COLON' : 8, 'PSTART' : 9, 'PEND' : 10, 'QUESTION' : 11, 'SLASH': 12}\n",
        "\n",
        "MODELS = { 'bert-base-uncased': (BertModel, BertTokenizer, 768, 'bert') }"
      ],
      "metadata": {
        "id": "gv7EPolb9Knm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset augmentations supported"
      ],
      "metadata": {
        "id": "0UMDxMW2RNRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpha_sub = 0.40\n",
        "alpha_del = 0.40\n",
        "tokenizer = None\n",
        "sub_style = 'unk'\n",
        "\n",
        "\n",
        "def augment_none(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style):\n",
        "    x_aug.append(x[i])\n",
        "    y_aug.append(y[i])\n",
        "    y_mask_aug.append(y_mask[i])\n",
        "\n",
        "\n",
        "def augment_substitute(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style):\n",
        "    if sub_style == 'rand':\n",
        "        x_aug.append(np.random.randint(tokenizer.vocab_size))\n",
        "    else:\n",
        "        x_aug.append(TOKEN_IDX[token_style]['UNK'])\n",
        "    y_aug.append(y[i])\n",
        "    y_mask_aug.append(y_mask[i])\n",
        "\n",
        "\n",
        "def augment_insert(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style):\n",
        "    x_aug.append(TOKEN_IDX[token_style]['UNK'])\n",
        "    y_aug.append(0)\n",
        "    y_mask_aug.append(1)\n",
        "    x_aug.append(x[i])\n",
        "    y_aug.append(y[i])\n",
        "    y_mask_aug.append(y_mask[i])\n",
        "\n",
        "\n",
        "def augment_delete(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style):\n",
        "    return\n",
        "\n",
        "\n",
        "def augment_all(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style):\n",
        "    r = np.random.rand()\n",
        "    if r < alpha_sub:\n",
        "        augment_substitute(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style)\n",
        "    elif r < alpha_sub + alpha_del:\n",
        "        augment_delete(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style)\n",
        "    else:\n",
        "        augment_insert(x, y, y_mask, x_aug, y_aug, y_mask_aug, i, token_style)\n",
        "\n",
        "\n",
        "# supported augmentation techniques\n",
        "AUGMENTATIONS = {\n",
        "    'none': augment_none,\n",
        "    'substitute': augment_substitute,\n",
        "    'insert': augment_insert,\n",
        "    'delete': augment_delete,\n",
        "    'all': augment_all\n",
        "}"
      ],
      "metadata": {
        "id": "U5NkPtLZ_R0h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "NE-YowjP7vUQ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataloader"
      ],
      "metadata": {
        "id": "AMrlRcmuQ-9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_data(dset, tokenizer, sequence_len, token_style):\n",
        "    data_items = []\n",
        "    while idx < len (dset):\n",
        "        x = [TOKEN_IDX[token_style]['START_SEQ']]\n",
        "        y = [0]\n",
        "        y_mask = [1]\n",
        "\n",
        "        while len (x) < sequence_len - 1 and idx < len (dset):\n",
        "            word, punc = dset[idx]\n",
        "            tokens = tokenizer.tokenize(word)\n",
        "            if len(tokens) + len(x) >= sequence_len:\n",
        "                break\n",
        "            else:\n",
        "                for i in range(len(tokens) - 1):\n",
        "                    x.append(tokenizer.convert_tokens_to_ids(tokens[i]))\n",
        "                    y.append(0)\n",
        "                    y_mask.append(0)\n",
        "                if len(tokens) > 0:\n",
        "                    x.append(tokenizer.convert_tokens_to_ids(tokens[-1]))\n",
        "                else:\n",
        "                    x.append(TOKEN_IDX[token_style]['UNK'])\n",
        "                y.append(punctuation_dict[punc])\n",
        "                y_mask.append(1)\n",
        "                idx += 1\n",
        "            x.append(TOKEN_IDX[token_style]['END_SEQ'])\n",
        "            y.append(0)\n",
        "            y_mask.append(1)\n",
        "            if len(x) < sequence_len:\n",
        "                x = x + [TOKEN_IDX[token_style]['PAD'] for _ in range(sequence_len - len(x))]\n",
        "                y = y + [0 for _ in range(sequence_len - len(y))]\n",
        "                y_mask = y_mask + [0 for _ in range(sequence_len - len(y_mask))]\n",
        "            attn_mask = [1 if token != TOKEN_IDX[token_style]['PAD'] else 0 for token in x]\n",
        "            data_items.append([x, y, attn_mask, y_mask])\n",
        "    return data_items\n",
        "\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, tokenizer, sequence_len, token_style, is_train=False, augment_rate=0.1,\n",
        "                 augment_type='substitute'):\n",
        "        self.data = parse_data(files, tokenizer, sequence_len, token_style)\n",
        "        self.sequence_len = sequence_len\n",
        "        self.augment_rate = augment_rate\n",
        "        self.token_style = token_style\n",
        "        self.is_train = is_train\n",
        "        self.augment_type = augment_type\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def _augment(self, x, y, y_mask):\n",
        "        x_aug = []\n",
        "        y_aug = []\n",
        "        y_mask_aug = []\n",
        "        for i in range(len(x)):\n",
        "            r = np.random.rand()\n",
        "            if r < self.augment_rate:\n",
        "                AUGMENTATIONS[self.augment_type](x, y, y_mask, x_aug, y_aug, y_mask_aug, i, self.token_style)\n",
        "            else:\n",
        "                x_aug.append(x[i])\n",
        "                y_aug.append(y[i])\n",
        "                y_mask_aug.append(y_mask[i])\n",
        "\n",
        "        if len(x_aug) > self.sequence_len:\n",
        "            # len increased due to insert\n",
        "            x_aug = x_aug[0:self.sequence_len]\n",
        "            y_aug = y_aug[0:self.sequence_len]\n",
        "            y_mask_aug = y_mask_aug[0:self.sequence_len]\n",
        "        elif len(x_aug) < self.sequence_len:\n",
        "            # len decreased due to delete\n",
        "            x_aug = x_aug + [TOKEN_IDX[self.token_style]['PAD'] for _ in range(self.sequence_len - len(x_aug))]\n",
        "            y_aug = y_aug + [0 for _ in range(self.sequence_len - len(y_aug))]\n",
        "            y_mask_aug = y_mask_aug + [0 for _ in range(self.sequence_len - len(y_mask_aug))]\n",
        "\n",
        "        attn_mask = [1 if token != TOKEN_IDX[self.token_style]['PAD'] else 0 for token in x]\n",
        "        return x_aug, y_aug, attn_mask, y_mask_aug\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index][0]\n",
        "        y = self.data[index][1]\n",
        "        attn_mask = self.data[index][2]\n",
        "        y_mask = self.data[index][3]\n",
        "\n",
        "        if self.is_train and self.augment_rate > 0:\n",
        "            x, y, attn_mask, y_mask = self._augment(x, y, y_mask)\n",
        "\n",
        "        x = torch.tensor(x)\n",
        "        y = torch.tensor(y)\n",
        "        attn_mask = torch.tensor(attn_mask)\n",
        "        y_mask = torch.tensor(y_mask)\n",
        "\n",
        "        return x, y, attn_mask, y_mask\n"
      ],
      "metadata": {
        "id": "p7evPVz88Xxn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create model using BERT layers"
      ],
      "metadata": {
        "id": "f-Z3IbULQ64Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class DeepPunctuation(nn.Module):\n",
        "    def __init__(self, pretrained_model, freeze_bert=False, lstm_dim=-1):\n",
        "        super(DeepPunctuation, self).__init__()\n",
        "        self.output_dim = len(punctuation_dict)\n",
        "        self.bert_layer = MODELS[pretrained_model][0].from_pretrained(pretrained_model)\n",
        "        # Freeze bert layers\n",
        "        if freeze_bert:\n",
        "            for p in self.bert_layer.parameters():\n",
        "                p.requires_grad = False\n",
        "        bert_dim = MODELS[pretrained_model][2]\n",
        "        if lstm_dim == -1:\n",
        "            hidden_size = bert_dim\n",
        "        else:\n",
        "            hidden_size = lstm_dim\n",
        "        self.lstm = nn.LSTM(input_size=bert_dim, hidden_size=hidden_size, num_layers=1, bidirectional=True)\n",
        "        self.linear = nn.Linear(in_features=hidden_size*2, out_features=len(punctuation_dict))\n",
        "\n",
        "    def forward(self, x, attn_masks):\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.view(1, x.shape[0])  # add dummy batch for single sample\n",
        "        # (B, N, E) -> (B, N, E)\n",
        "        x = self.bert_layer(x, attention_mask=attn_masks)[0]\n",
        "        # (B, N, E) -> (N, B, E)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x, (_, _) = self.lstm(x)\n",
        "        # (N, B, E) -> (B, N, E)\n",
        "        x = torch.transpose(x, 0, 1)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "GeRaZenU_g2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create splits of dataset"
      ],
      "metadata": {
        "id": "rmgeJwsAQ3Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = tagged[:550000]\n",
        "test_data = tagged[550000:]\n",
        "valid_data = train_data[:100000]\n",
        "train_data = train_data[100000:]"
      ],
      "metadata": {
        "id": "DEViAo-aCjcv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rest of the assignment could not be completed in the given time"
      ],
      "metadata": {
        "id": "ZcTAGLejRuqq"
      }
    }
  ]
}